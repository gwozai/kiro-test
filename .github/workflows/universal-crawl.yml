name: é€šç”¨ç½‘é¡µæŠ“å–

on:
  workflow_dispatch:
    inputs:
      urls:
        description: 'è¦æŠ“å–çš„URLï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰'
        required: true
        default: 'https://example.com'
      wait_time:
        description: 'ç­‰å¾…æ—¶é—´(ç§’)ï¼Œç”¨äºç»•è¿‡éªŒè¯'
        required: false
        default: '5'

permissions:
  contents: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: å®‰è£…ä¾èµ–
        run: |
          npm install playwright
          npx playwright install chromium --with-deps

      - name: åˆ›å»ºæŠ“å–è„šæœ¬
        run: |
          cat > crawl.js << 'EOF'
          const { chromium } = require('playwright');
          const fs = require('fs');

          const URLS = process.env.URLS.split(',').map(u => u.trim());
          const WAIT_TIME = parseInt(process.env.WAIT_TIME) || 5;
          const OUTPUT_DIR = 'crawled_data';

          if (!fs.existsSync(OUTPUT_DIR)) {
            fs.mkdirSync(OUTPUT_DIR, { recursive: true });
          }

          async function crawlUrl(browser, url, index) {
            const context = await browser.newContext({
              userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            });
            const page = await context.newPage();
            
            try {
              console.log(`\nğŸ”„ æŠ“å–: ${url}`);
              
              await page.goto(url, { waitUntil: 'networkidle', timeout: 60000 });
              
              // ç­‰å¾…é¡µé¢åŠ è½½å’Œå¯èƒ½çš„éªŒè¯
              console.log(`â³ ç­‰å¾… ${WAIT_TIME} ç§’...`);
              await page.waitForTimeout(WAIT_TIME * 1000);
              
              const title = await page.title();
              console.log(`ğŸ“„ æ ‡é¢˜: ${title}`);
              
              // æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é¡µé¢
              if (title.includes('è¯·ç¨å€™') || title.includes('Cloudflare') || title.includes('éªŒè¯')) {
                console.log('âš ï¸ æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼Œå°è¯•ç­‰å¾…æ›´é•¿æ—¶é—´...');
                await page.waitForTimeout(10000);
              }
              
              const content = await page.evaluate(() => {
                // ç§»é™¤è„šæœ¬å’Œæ ·å¼
                document.querySelectorAll('script, style, nav, header, footer, .ad, .ads').forEach(el => el.remove());
                return document.body.innerText;
              });

              const result = {
                url: url,
                title: title,
                crawled_at: new Date().toISOString(),
                content_length: content.length,
                content_preview: content.substring(0, 500) + '...'
              };

              const filename = `${OUTPUT_DIR}/url_${index + 1}.json`;
              fs.writeFileSync(filename, JSON.stringify(result, null, 2), 'utf8');
              
              // ä¿å­˜å®Œæ•´æ–‡æœ¬
              fs.writeFileSync(`${OUTPUT_DIR}/url_${index + 1}.txt`, content, 'utf8');
              
              console.log(`âœ… å®Œæˆï¼Œå†…å®¹é•¿åº¦: ${content.length} å­—ç¬¦`);
              return { success: true, url, title };
              
            } catch (error) {
              console.error(`âŒ å¤±è´¥: ${error.message}`);
              return { success: false, url, error: error.message };
            } finally {
              await context.close();
            }
          }

          async function main() {
            console.log(`ğŸš€ å¼€å§‹æŠ“å– ${URLS.length} ä¸ªURL\n`);
            
            const browser = await chromium.launch({ headless: true });
            const results = [];

            for (let i = 0; i < URLS.length; i++) {
              const result = await crawlUrl(browser, URLS[i], i);
              results.push(result);
            }

            await browser.close();

            const summary = {
              total: URLS.length,
              success: results.filter(r => r.success).length,
              failed: results.filter(r => !r.success).length,
              results: results
            };

            fs.writeFileSync(`${OUTPUT_DIR}/summary.json`, JSON.stringify(summary, null, 2));
            
            console.log(`\nâœ¨ å®Œæˆ! æˆåŠŸ: ${summary.success}, å¤±è´¥: ${summary.failed}`);
          }

          main().catch(console.error);
          EOF

      - name: æ‰§è¡ŒæŠ“å–
        env:
          URLS: ${{ github.event.inputs.urls }}
          WAIT_TIME: ${{ github.event.inputs.wait_time }}
        run: node crawl.js

      - name: æäº¤åˆ°ä»“åº“
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add crawled_data/
          git commit -m "data: æŠ“å–ç½‘é¡µå†…å®¹ [skip ci]" || echo "No changes"
          git push
