name: æ‰¹é‡æŠ“å–ç½‘é¡µ

on:
  workflow_dispatch:
    inputs:
      start_page:
        description: 'èµ·å§‹é¡µç '
        required: true
        default: '1'
      end_page:
        description: 'ç»“æŸé¡µç '
        required: true
        default: '455'
      concurrency:
        description: 'å¹¶å‘æ•°'
        required: true
        default: '10'

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: å®‰è£…ä¾èµ–
        run: |
          sudo apt-get update && sudo apt-get install -y html2text
          npm install playwright
          npx playwright install chromium --with-deps

      - name: åˆ›å»ºæŠ“å–è„šæœ¬
        run: |
          cat > crawl.js << 'EOF'
          const { chromium } = require('playwright');
          const fs = require('fs');
          const path = require('path');

          const START_PAGE = parseInt(process.env.START_PAGE) || 1;
          const END_PAGE = parseInt(process.env.END_PAGE) || 455;
          const CONCURRENCY = parseInt(process.env.CONCURRENCY) || 10;
          const BASE_URL = 'https://www.nihaowua.com';
          const OUTPUT_DIR = 'crawled_data';

          // åˆ›å»ºè¾“å‡ºç›®å½•
          if (!fs.existsSync(OUTPUT_DIR)) {
            fs.mkdirSync(OUTPUT_DIR, { recursive: true });
          }

          async function crawlPage(browser, pageNum) {
            const context = await browser.newContext();
            const page = await context.newPage();
            
            try {
              const url = pageNum === 1 ? BASE_URL : `${BASE_URL}/page/${pageNum}/`;
              console.log(`ğŸ”„ æŠ“å–ç¬¬ ${pageNum} é¡µ: ${url}`);
              
              await page.goto(url, { waitUntil: 'domcontentloaded', timeout: 30000 });
              
              // æå–æ–‡ç« åˆ—è¡¨
              const articles = await page.evaluate(() => {
                const items = [];
                document.querySelectorAll('article, .post, .entry, h2 a, .post-title a').forEach(el => {
                  const link = el.querySelector('a') || el;
                  if (link.href && link.textContent) {
                    items.push({
                      title: link.textContent.trim().substring(0, 100),
                      url: link.href
                    });
                  }
                });
                return items.slice(0, 20); // æ¯é¡µæœ€å¤š20æ¡
              });

              const content = await page.evaluate(() => document.body.innerText.substring(0, 5000));
              
              const result = {
                page: pageNum,
                url: url,
                crawled_at: new Date().toISOString(),
                articles: articles,
                content_preview: content.substring(0, 2000)
              };

              // ä¿å­˜åˆ°æ–‡ä»¶
              const filename = path.join(OUTPUT_DIR, `page_${String(pageNum).padStart(4, '0')}.json`);
              fs.writeFileSync(filename, JSON.stringify(result, null, 2), 'utf8');
              
              console.log(`âœ… ç¬¬ ${pageNum} é¡µå®Œæˆï¼Œæ‰¾åˆ° ${articles.length} ç¯‡æ–‡ç« `);
              return { success: true, page: pageNum, articles: articles.length };
              
            } catch (error) {
              console.error(`âŒ ç¬¬ ${pageNum} é¡µå¤±è´¥: ${error.message}`);
              return { success: false, page: pageNum, error: error.message };
            } finally {
              await context.close();
            }
          }

          async function main() {
            console.log(`\nğŸš€ å¼€å§‹æ‰¹é‡æŠ“å–`);
            console.log(`ğŸ“„ é¡µç èŒƒå›´: ${START_PAGE} - ${END_PAGE}`);
            console.log(`âš¡ å¹¶å‘æ•°: ${CONCURRENCY}\n`);

            const browser = await chromium.launch();
            const pages = [];
            
            for (let i = START_PAGE; i <= END_PAGE; i++) {
              pages.push(i);
            }

            const results = [];
            
            // åˆ†æ‰¹å¹¶å‘å¤„ç†
            for (let i = 0; i < pages.length; i += CONCURRENCY) {
              const batch = pages.slice(i, i + CONCURRENCY);
              const batchResults = await Promise.all(
                batch.map(pageNum => crawlPage(browser, pageNum))
              );
              results.push(...batchResults);
              
              // è¿›åº¦
              const done = Math.min(i + CONCURRENCY, pages.length);
              console.log(`\nğŸ“Š è¿›åº¦: ${done}/${pages.length} (${Math.round(done/pages.length*100)}%)\n`);
            }

            await browser.close();

            // ç”Ÿæˆæ±‡æ€»
            const summary = {
              total_pages: pages.length,
              success: results.filter(r => r.success).length,
              failed: results.filter(r => !r.success).length,
              crawled_at: new Date().toISOString(),
              failed_pages: results.filter(r => !r.success).map(r => r.page)
            };

            fs.writeFileSync(
              path.join(OUTPUT_DIR, 'summary.json'),
              JSON.stringify(summary, null, 2),
              'utf8'
            );

            console.log(`\nâœ¨ æŠ“å–å®Œæˆ!`);
            console.log(`   æˆåŠŸ: ${summary.success} é¡µ`);
            console.log(`   å¤±è´¥: ${summary.failed} é¡µ`);
          }

          main().catch(console.error);
          EOF

      - name: æ‰§è¡ŒæŠ“å–
        env:
          START_PAGE: ${{ github.event.inputs.start_page }}
          END_PAGE: ${{ github.event.inputs.end_page }}
          CONCURRENCY: ${{ github.event.inputs.concurrency }}
        run: node crawl.js

      - name: æäº¤åˆ°ä»“åº“
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add crawled_data/
          git commit -m "data: æŠ“å– nihaowua.com ç¬¬ ${{ github.event.inputs.start_page }}-${{ github.event.inputs.end_page }} é¡µ [skip ci]" || echo "No changes"
          git push
